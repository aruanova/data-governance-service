# Panel-Datainspector - Plan de RefactorizaciÃ³n (Monorepo)

## ðŸ“‹ Estado Actual
- **Stack Backend**: FastAPI + Celery + Redis + PostgreSQL + OpenAI
- **Stack Frontend**: Next.js 15.3.5 + React 19 + TypeScript
- **PropÃ³sito**: ClasificaciÃ³n de lÃ­neas CFDI usando LLM con refinamiento iterativo
- **Complejidad**: 12 routers, 25+ servicios, procesamiento async distribuido

## ðŸŽ¯ Objetivos de la RefactorizaciÃ³n
1. Migrar backend de Python/FastAPI a Go
2. Cambiar a Svelte con Skeleton (solo ajustar endpoints si es necesario)
3. Estructura monorepo para facilitar desarrollo
4. Mejorar performance y reducir uso de memoria
5. Simplificar arquitectura manteniendo funcionalidad completa

## ðŸ—ï¸ Estructura Monorepo Propuesta

```
data-governance-service/
â”œâ”€â”€ .claude/                    # DocumentaciÃ³n y contexto de Claude
â”‚   â”œâ”€â”€ PLANNING.md
â”‚   â””â”€â”€ CLAUDE.md
â”œâ”€â”€ backend/                    # Backend en Go
â”‚   â”œâ”€â”€ cmd/
â”‚   â”‚   â”œâ”€â”€ api/               # API REST server
â”‚   â”‚   â””â”€â”€ worker/            # Worker para procesamiento async
â”‚   â”œâ”€â”€ internal/
â”‚   â”‚   â”œâ”€â”€ api/               # HTTP handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ handlers/      # Controladores REST
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware/    # Auth, CORS, logging
â”‚   â”‚   â”‚   â””â”€â”€ websocket/     # WebSocket handlers
â”‚   â”‚   â”œâ”€â”€ core/              # Dominio central
â”‚   â”‚   â”‚   â”œâ”€â”€ models/        # Modelos de dominio
â”‚   â”‚   â”‚   â”œâ”€â”€ ports/         # Interfaces (hexagonal)
â”‚   â”‚   â”‚   â””â”€â”€ services/      # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ infrastructure/    # Implementaciones externas
â”‚   â”‚   â”‚   â”œâ”€â”€ database/      # PostgreSQL
â”‚   â”‚   â”‚   â”œâ”€â”€ cache/         # Redis
â”‚   â”‚   â”‚   â”œâ”€â”€ storage/       # File system / S3
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/           # OpenAI client
â”‚   â”‚   â”‚   â””â”€â”€ queue/         # Task queue (Asynq)
â”‚   â”‚   â””â”€â”€ pkg/               # Utilidades compartidas
â”‚   â”œâ”€â”€ migrations/            # SQL migrations
â”‚   â”œâ”€â”€ configs/               # Configuraciones
â”‚   â””â”€â”€ tests/                 # Tests de integraciÃ³n
â”œâ”€â”€ frontend/                  # Frontend (A definir si Svelte)
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ lib/
â”‚   â””â”€â”€ public/
â”œâ”€â”€ shared/                    # Compartido entre front y back
â”‚   â”œâ”€â”€ types/                # TypeScript types que matchean Go structs
â”‚   â””â”€â”€ api-spec/             # OpenAPI/Swagger specs
â”œâ”€â”€ scripts/                   # Scripts de build y deployment
â”œâ”€â”€ docker/                    # Dockerfiles y compose
â”œâ”€â”€ .github/                   # CI/CD workflows
â””â”€â”€ docs/                      # DocumentaciÃ³n del proyecto
```

## ðŸ”„ Servicios Go Propuestos

### 1. API Service (backend/cmd/api)
- **Responsabilidad**: Exponer REST API, manejar uploads, WebSockets
- **Puertos**: 8080 (HTTP), 8081 (WebSocket)
- **Similar a routers Python**: pipeline.py, cleaning.py, validation.py, batch.py

### 2. Worker Service (backend/cmd/worker)
- **Responsabilidad**: Procesar tareas async, LLM calls, batch processing
- **Similar a**: Celery workers actuales
- **Queue**: Asynq (Redis-based) o Temporal
- **Resiliencia**: Checkpointing, health checks, graceful shutdown

### 3. Servicios de Dominio (backend/internal/core/services)
```
services/
â”œâ”€â”€ upload/              # GestiÃ³n de uploads y archivos ZIP
â”œâ”€â”€ schema_inspector/    # InspecciÃ³n y anÃ¡lisis de archivos
â”œâ”€â”€ refinery/           # Sistema modular de limpieza de texto
â”‚   â”œâ”€â”€ v1_standard/    # VersiÃ³n para datos mexicanos
â”‚   â”œâ”€â”€ v2_aggressive/  # Limpieza mÃ¡s agresiva
â”‚   â””â”€â”€ registry/       # Registro de versiones
â”œâ”€â”€ cleaning/           # Limpieza y deduplicaciÃ³n
â”‚   â”œâ”€â”€ deduplication/  # Sistema two-level de dedup
â”‚   â””â”€â”€ json_generator/ # Generador de JSON para LLM
â”œâ”€â”€ prompts/            # GestiÃ³n de prompts customizables
â”‚   â”œâ”€â”€ storage/        # Almacenamiento de prompts
â”‚   â””â”€â”€ versioning/     # Versionado de prompts
â”œâ”€â”€ llm/               # ClasificaciÃ³n con LLM
â”œâ”€â”€ validation/        # Muestreo y validaciÃ³n
â”œâ”€â”€ batch/             # Procesamiento multi-archivo
â”œâ”€â”€ iteration/         # Tracking de iteraciones
â”œâ”€â”€ metrics/           # Captura de mÃ©tricas
â”œâ”€â”€ checkpoint/        # Sistema de checkpointing
â”œâ”€â”€ recovery/          # RecuperaciÃ³n de trabajos fallidos
â””â”€â”€ session/           # GestiÃ³n de sesiones
```

## ðŸ“Š Flujo de Procesamiento de Archivos

### Pipeline Pre-LLM
```mermaid
graph LR
    A[Upload ZIP] --> B[Schema Inspection]
    B --> C[Refinery Cleaning]
    C --> D[Deduplication]
    D --> E[JSON Generation]
    E --> F[LLM Processing]
```

### Etapas Detalladas

#### 1. Upload & ExtracciÃ³n
- RecepciÃ³n de archivos ZIP con mÃºltiples formatos
- Soporte para: CSV, XLSX, JSON, JSONL, NDJSON
- Streaming para archivos grandes
- ValidaciÃ³n de esquemas

#### 2. Schema Inspection
- DetecciÃ³n automÃ¡tica de columnas
- Inferencia de tipos de datos
- AnÃ¡lisis de calidad de datos
- Sampling para preview

#### 3. Refinery (Sistema Modular)
- **Arquitectura Plugin**: Versiones intercambiables
- **v1-standard**: Limpieza para datos mexicanos
- **v2-aggressive**: Limpieza mÃ¡s estricta
- **Nodos de Procesamiento**:
  - Remover cÃ³digos con prefijos (ART###)
  - Remover meses en espaÃ±ol
  - NormalizaciÃ³n de caracteres especiales
  - Remover palabras cortas
  - Aplicar whitelist/blacklist

#### 4. DeduplicaciÃ³n Two-Level
- **Nivel 1**: DeduplicaciÃ³n dentro del batch
- **Nivel 2**: DeduplicaciÃ³n universal (cross-session)
- Estrategias: exact, fuzzy, universal
- Usa columnas "clean" generadas por Refinery

#### 5. JSON Generation
- CreaciÃ³n de estructura optimizada para LLM
- InclusiÃ³n de _row_index para tracking
- Solo campos "clean" para reducir tokens
- Metadata para contexto

## ðŸ¤– Sistema Multi-Proveedor LLM

### Proveedores Soportados
- **OpenAI**: GPT-4, GPT-4o-mini, GPT-3.5-turbo
- **Google Gemini**: Gemini-Pro, Gemini-1.5-Pro
- **Extensible**: Factory pattern para agregar nuevos proveedores

### CaracterÃ­sticas
- **SelecciÃ³n DinÃ¡mica**: Usuario escoge proveedor por request
- **Fallback AutomÃ¡tico**: Si un proveedor falla, notificar al usuario y darle a escoger usar una alternativa
- **ConfiguraciÃ³n por Proveedor**:
  - API Keys independientes
  - Modelos especÃ­ficos
  - Rate limits y retry policies
- **MÃ©tricas Comparativas**:
  - Tiempo de respuesta
  - Costo por tokens
  - Calidad de clasificaciÃ³n

## ðŸŽ¨ Sistema de Prompts Customizables

### CaracterÃ­sticas Clave
- **100% Customizable**: NO hay prompt default
- **GestiÃ³n de Prompts**:
  - Crear, editar, eliminar prompts
  - Asignar labels descriptivos
  - Versionado automÃ¡tico
  - Compartir entre usuarios
- **CategorÃ­as DinÃ¡micas**:
  - Usuario define sus propias categorÃ­as
  - Puede importar/exportar sets de categorÃ­as
  - Prioridades configurables
- **Storage**:
  - PostgreSQL para persistencia
  - Redis para cache
  - Historial de versiones

### Estructura de Prompt
```json
{
  "id": "uuid",
  "name": "Mi Prompt OXXO v3",
  "label": "ClasificaciÃ³n gastos OXXO 2024",
  "template": "Texto del prompt customizado...",
  "categories": [
    {
      "id": 1,
      "name": "CategorÃ­a Custom 1",
      "description": "DescripciÃ³n",
      "keywords": ["palabra1", "palabra2"]
    }
  ],
  "created_by": "user_id",
  "version": 3,
  "is_active": true
}
```

## ðŸ“Š Fases de MigraciÃ³n

### Fase 0: Setup Monorepo (Semana 1)
- [x] Crear estructura .claude
- [ ] Definir estructura de carpetas
- [ ] Setup Go modules
- [ ] Configurar herramientas (linters, formatters)
- [ ] Docker Compose para desarrollo

### Fase 1: Core Infrastructure (Semana 2)
- [ ] ConexiÃ³n PostgreSQL con pgx
- [ ] ConexiÃ³n Redis
- [ ] Logging estructurado (slog)
- [ ] Config management (viper)
- [ ] Migrations setup (golang-migrate)

### Fase 2: Upload & Cleaning Pipeline (Semana 3-4)
- [ ] Upload de archivos (Excel/CSV/JSON)
- [ ] Parsing streaming de archivos grandes
- [ ] Pipeline de limpieza (refinery v3)
- [ ] DeduplicaciÃ³n universal
- [ ] GeneraciÃ³n de JSON para LLM

### Fase 3: LLM Integration (Semana 5-6)
- [ ] Cliente OpenAI
- [ ] Cliente Gemini
- [ ] Chunking dinÃ¡mico configurable (LLM_DISTRIBUTED_CHUNK_SIZE)
- [ ] Worker pool configurable para procesamiento paralelo
- [ ] Manejo de respuestas y normalizaciÃ³n
- [ ] Count mismatch handling
- [ ] Retry logic con backoff

### Fase 4: Task Queue System (Semana 7-8)
- [ ] Setup Asynq o Temporal
- [ ] Migrar lÃ³gica de Celery tasks
- [ ] Progress tracking
- [ ] WebSocket updates

### Fase 5: Validation & Refinement (Semana 9-10)
- [ ] Sampling strategies
- [ ] Validation submission
- [ ] Prompt refinement
- [ ] Iteration tracking
- [ ] Comparative metrics

### Fase 6: Batch Processing (Semana 11-12)
- [ ] Directory scanning
- [ ] Schema validation
- [ ] Multi-file processing
- [ ] Consolidation
- [ ] Streaming para archivos grandes

## ðŸ› ï¸ Stack TecnolÃ³gico Definitivo

### Backend (Go)
- **Web Framework**: Gin
- **Task Queue**: Asynq (simple, Redis)
- **Database**: GORM (ORM completo)
- **Cache**: go-redis/v9
- **LLM Providers**:
  - OpenAI: sashabaranov/go-openai
  - Gemini: google/generative-ai-go
  - Factory Pattern para multi-proveedor
- **Excel**: excelize/v2
- **Validation**: go-playground/validator/v10
- **Config**: spf13/viper
- **Logging**: slog (stdlib)
- **Metrics**: Prometheus

### Infraestructura
- **PostgreSQL**: v15+ (igual que actual)
- **Redis**: v7+ (igual que actual)
- **Docker**: Multi-stage builds
- **CI/CD**: GitHub Actions

## â“ Decisiones TÃ©cnicas Pendientes

### Alta Prioridad
1. **Web Framework**:
   - Gin (popular, buen balance)

2. **Task Queue**:
   - Asynq (simple, suficiente para el caso)

3. **SQL Approach**:
   - GORM (ORM completo)

### Media Prioridad
4. **Estructura del cÃ³digo**:
   - Hexagonal/Clean Architecture

5. **Testing Strategy**:
   - Â¿testify para assertions?
   - Â¿mockery para mocks?
   - Â¿testcontainers para integration?

## ðŸŽ¯ Optimizaciones Clave vs Python

### Performance
```go
// Streaming de archivos grandes sin cargar en memoria
func ProcessLargeFile(reader io.Reader) error {
    scanner := bufio.NewScanner(reader)
    batch := make([]Record, 0, 1000)

    for scanner.Scan() {
        record := ParseRecord(scanner.Text())
        batch = append(batch, record)

        if len(batch) >= 1000 {
            processBatch(batch)
            batch = batch[:0] // reuse slice
        }
    }
}
```

### Concurrencia Real
```go
// Procesamiento paralelo de chunks con workers pool
func ProcessChunks(chunks []Chunk) {
    workers := runtime.NumCPU()
    ch := make(chan Chunk, len(chunks))
    var wg sync.WaitGroup

    // Worker pool
    for i := 0; i < workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for chunk := range ch {
                processWithLLM(chunk)
            }
        }()
    }

    // Feed work
    for _, chunk := range chunks {
        ch <- chunk
    }
    close(ch)
    wg.Wait()
}
```

## ðŸ“ˆ MÃ©tricas de Ã‰xito
- [ ] 50% reducciÃ³n en uso de memoria
- [ ] 2x mejora en throughput
- [ ] <100ms latencia P95 en API
- [ ] 80% cobertura de tests
- [ ] Zero downtime durante migraciÃ³n

## ðŸš€ PrÃ³ximos Pasos Inmediatos
1. âœ… Crear estructura .claude
2. Definir decisiones tÃ©cnicas clave
3. Crear CLAUDE.md con convenciones
4. Setup inicial del proyecto Go
5. Implementar primer endpoint como POC

## ðŸ“ Notas Importantes
- Frontend se rediseÃ±a con Svelte/Skeleton
- API debe mantener compatibilidad con frontend actual
- Aprovechar monorepo para compartir tipos/specs
- Deployment sera usa